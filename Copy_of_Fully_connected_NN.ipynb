{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMFmiqn8Y4n1OUPKWwrA/mK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangling297/MAT_CS_599_deepLearningClassPracitices/blob/main/Copy_of_Fully_connected_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oZ8HnT9clNk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building convolutional Neural Networks from Scratch"
      ],
      "metadata": {
        "id": "4oFWjmprQgN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fully- connected NN for binary classification - using heart disease data - upload the data, subset the columns to"
      ],
      "metadata": {
        "id": "ZIc1rc0wcrRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import default_banner\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "# When training Deep Learning models, randomness enters the process in a few different places\n",
        "\n",
        "  # 1.starting values for the weights(the opitmizer will try to improve these weights)\n",
        "  # 2. The order in which we process the minibatches when we do SGD\n",
        "  # 3. When we split the data into Train, Validation, Test etc\n",
        "  # 4. Dropout(if using regularization)\n",
        "\n",
        "# Set the seed for different random number generators so that the results will be the same each time  the notebook is run"
      ],
      "metadata": {
        "id": "t2RlfX4Wc_VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(42)"
      ],
      "metadata": {
        "id": "6VEpQt6JS2CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/heart_disease.csv')"
      ],
      "metadata": {
        "id": "TTRhTpo2TM_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "fIpMbvBJTcIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "a7RDrtB3UMwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# check 1's and 0 's to see if they are balanced."
      ],
      "metadata": {
        "id": "kQH5EZqwUeh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.target.value_counts(normalize=True, dropna=False)"
      ],
      "metadata": {
        "id": "2UyvG2FyUmV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing # Collet groups variables into two lists"
      ],
      "metadata": {
        "id": "Umy2gsoyV0Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_Variables = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'ca', 'thal']\n",
        "numerics = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope']"
      ],
      "metadata": {
        "id": "b_wUXNdsVr_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   one-hot encode the categorical variables with the pandas get_dummies function\n",
        "\n",
        "*   Normalize the numeric varibles\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7AIsK1CXAcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns = categorical_Variables)"
      ],
      "metadata": {
        "id": "O3jQfceRXT4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "BDzaC-8PXli4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NNs work best when the inputs are all roughly in the same range. So standard practice is to standardize the numeric varibles; Split the data into 80% training and 20% testing set before normalization"
      ],
      "metadata": {
        "id": "xDjBi1pWXzu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = df.sample(frac=0.2, random_state=42)\n",
        "train_df = df.drop(test_df.index)"
      ],
      "metadata": {
        "id": "RAVmbAlDYJW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "WbpP3di4Ykkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape"
      ],
      "metadata": {
        "id": "OZaRGFSkYqgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the mean and standard deviation of every numeric variable in the training set."
      ],
      "metadata": {
        "id": "R1miCqfWYxic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "means = train_df[numerics].mean()\n",
        "sd = train_df[numerics].std()"
      ],
      "metadata": {
        "id": "q7CZGRRoY7BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "means"
      ],
      "metadata": {
        "id": "N118t4M9ZKrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize the train and test dataframes with these means and standard deviations"
      ],
      "metadata": {
        "id": "5uCpyMKxZsA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[numerics] = (train_df[numerics] - means)/sd"
      ],
      "metadata": {
        "id": "iFtqwT-CZz53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[numerics] = (test_df[numerics]- means)/sd"
      ],
      "metadata": {
        "id": "mDBTWF3baEBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "YEKMcgM6aPdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So at this point, the whole dataset is numeric. Now feed data to keras/Tensorflow is as Numpy arrays so to convert our two dataframes to Numpy arrays"
      ],
      "metadata": {
        "id": "5jwHNvfPabfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_df.to_numpy()\n",
        "test = test_df.to_numpy()"
      ],
      "metadata": {
        "id": "72pUcHWVaw_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final step, features X and dependent variable y are both inside the train and test arrays so to seperate them out as the target column is y variable (counting from 0). The np.delete function is for selecting all columns (total 6) except one."
      ],
      "metadata": {
        "id": "mQGOTFNnbCv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = np.delete(train, 6, axis=1).astype(np.float32)\n",
        "test_X = np.delete(test, 6, axis=1).astype(np.float32)"
      ],
      "metadata": {
        "id": "IY3IHQaybrvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape, test_X.shape"
      ],
      "metadata": {
        "id": "3xArbhbcbP9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, Select just the 6th column and define the train and test y variables."
      ],
      "metadata": {
        "id": "xqmnobiBcY2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = train[:, 6].astype(np.float32)\n",
        "test_y = test[:, 6].astype(np.float32)"
      ],
      "metadata": {
        "id": "VhAOIHlDcgP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape, test_y.shape"
      ],
      "metadata": {
        "id": "ZTSBgoBYcq_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a model\n",
        "\n",
        "*   Define model in Keras ## Start with a single hidden layer; Since this is a binary flassification problme, we will use a signmoid activation in the output layer\n",
        "*   \n",
        "\n"
      ],
      "metadata": {
        "id": "M_wuRJ2kc7qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_columns = train_X.shape[1]\n",
        "#define the input layer\n",
        "input = keras.Input(shape=(num_columns,))\n",
        "\n",
        "#feed the input vector to the hidden layler (can give names to each layer to help)\n",
        "#keep track. This doesn't affect the training\n",
        "h = keras.layers.Dense(16, activation='relu', name=\"Hidden\")(input)\n",
        "\n",
        "#feed the output of the hidden layer to the output layer\n",
        "output = keras.layers.Dense(1, activation='sigmoid', name='Output')(h)\n",
        "\n",
        "# tell Keras taht this (input, output) pair is your model\n",
        "model = keras.Model(input, output)\n",
        "\n",
        "# use model.command to get a quick overview of what have been defined"
      ],
      "metadata": {
        "id": "pp56uWT7djmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "w_MFzbsPf-ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(28 +1) *16 + (16 +1) *1"
      ],
      "metadata": {
        "id": "YiIKWu7XgG3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the network graphically as well using Keras' plot_model function"
      ],
      "metadata": {
        "id": "zUiyYxtdgV8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "9smuJdjpggZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Optimization Parameters"
      ],
      "metadata": {
        "id": "tF8TnTXbgzKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary: the model is defined, now need to tell Keras theree things:\n",
        "\n",
        "\n",
        "*   What loss function to use- output variable is binary, we can select the binary_crossentropy loss function\n",
        "*   Which optimizer to use - we will use a sibling of SGD called Adam which which is an excellent default choice\n",
        "*  What metrcs you want to report out_ in classfication problems accuracy is the metric\n",
        "\n"
      ],
      "metadata": {
        "id": "t4gvJkVEg3Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "77isKkEGhd1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "jwaCOzveixH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To kickoff training, 3 things have to be decided:\n",
        "\n",
        "1.   The bach size (32 is the default)\n",
        "2.   The number of epochs (i.e.., how many passes through the training data - usually 20-30 passes is a good starting point)\n",
        "3. whether uses a validation set. This decision will be good for overfitting detection and regularization via early stopping so we will ask Keras to automatically use 20% of the data points as a validation set\n",
        "\n"
      ],
      "metadata": {
        "id": "OWmAsLJAi0Y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J8ew3wKnj0Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**use model.fit to train the model**"
      ],
      "metadata": {
        "id": "DkzKwvNZj01_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store the output of the training process in history. It will make it easy later to investigate what happened during training"
      ],
      "metadata": {
        "id": "9YdwcpuVj_Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_X, #the array with the input X columns\n",
        "                    train_y, #the array witht he output y column\n",
        "                    epochs=300, #numberj of epochs to run\n",
        "                    batch_size=32, # number jof samples (ie data points) per batch\n",
        "                    verbose=1, # verbosity during training)\n",
        "                    validation_split=0.2) # use 20% of the data for validation"
      ],
      "metadata": {
        "id": "WpwMPfCTkPOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting metrics like loss and accuracy as a function of the # of epochs is a good way to understand how training has progressed."
      ],
      "metadata": {
        "id": "fk0rjTd_lSaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())"
      ],
      "metadata": {
        "id": "bAO_IBz3l5eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ub4FhcrImojT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6s7eLgymHl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QvV13508o0ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model to see how the model does on the test set"
      ],
      "metadata": {
        "id": "gAMH29hxpAFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.evaluate is a function used to calcualte the performance of your model on any dataset"
      ],
      "metadata": {
        "id": "YwG8pYoCpIwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_X, test_y)"
      ],
      "metadata": {
        "id": "MWg_3nSVpQLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting new data with the model"
      ],
      "metadata": {
        "id": "AB7ggPMIpaiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to save a Keras model and use it for prediction  \n",
        "\n",
        "SUmmary : pre-processing (one-hot encoding and normalization) - remember what pre-processing we did and carry that information (mean and variance of each variable) along with the model to correctly use the model in the future\n",
        "\n",
        "using Keras preprocessing layers."
      ],
      "metadata": {
        "id": "ofqe2tO7pgTO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lPoubrjopGgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyTaVXuDo-FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "_NGP6K3io9T8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K4Uu2fSGj8Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "his"
      ],
      "metadata": {
        "id": "xA7KPZRTiw5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "GhdVI8ZNitvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "H9vW1tj4is8r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ceyDvpQgwcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xJgV0wj9gUxE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qRctY6eZgxNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JMe0gFhAgRU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Y7Rf2340c62r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyt7akrvc197"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dEO3qSK_cYj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4j1FB7XRW5Ca"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "726682e3"
      },
      "source": [
        "If you see output indicating that a GPU is available, then TensorFlow is configured to use it. When you run your deep learning models, they should automatically leverage the GPU for faster computations."
      ]
    }
  ]
}